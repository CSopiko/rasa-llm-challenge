{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import fnmatch\n",
    "import argparse\n",
    "import base64\n",
    "import time\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "repo_owner = \"RasaHQ\"\n",
    "repo_name = \"rasa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_from_github_repo(owner, repo, token):\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/main?recursive=1\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\": \"application/vnd.github+json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        content = response.json()\n",
    "        return content[\"tree\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Error fetching repo contents: {response.status_code}\")\n",
    "    \n",
    "\n",
    "def fetch_md_contents(files):\n",
    "    md_contents = []\n",
    "    for file in files:\n",
    "        if file[\"type\"] == \"blob\" and fnmatch.fnmatch(file[\"path\"], \"*.mdx\"):\n",
    "            response = requests.get(file[\"url\"])\n",
    "            if response.status_code == 200:\n",
    "                content = response.json()[\"content\"]\n",
    "                decoded_content = base64.b64decode(content).decode('utf-8')\n",
    "                print(\"Fetching Content from \", file['path'])\n",
    "                md_contents.append(Document(page_content=decoded_content, metadata={\"source\": file['path']}))\n",
    "            else:\n",
    "                print(f\"Error downloading file {file['path']}: {response.status_code}\")\n",
    "    return md_contents\n",
    "\n",
    "\n",
    "def get_source_chunks(files):\n",
    "    print(\"In get_source_chunks ...\")\n",
    "    source_chunks = []\n",
    "    splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
    "    for source in fetch_md_contents(files):\n",
    "        for chunk in splitter.split_text(source.page_content):\n",
    "            source_chunks.append(Document(page_content=chunk, metadate=source.metadata))\n",
    "    return source_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_files = get_files_from_github_repo(repo_owner, repo_name, GITHUB_TOKEN)\n",
    "docs_path = [file for file in repo_files if file[\"type\"] == \"blob\" and fnmatch.fnmatch(file[\"path\"], \"*.mdx\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_path[60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_contents = fetch_md_contents(docs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('0_60_md_files.pkl', 'wb') as f:\n",
    "    pkl.dump(md_contents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_chunks = []\n",
    "splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=2048, chunk_overlap=0)\n",
    "for source in md_contents:\n",
    "    for chunk in splitter.split_text(source.page_content):\n",
    "        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'CHANGELOG.mdx'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_chunks[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DB_PATH = f'./chroma/{os.path.basename(repo_name)}'\n",
    "\n",
    "chroma_db = None\n",
    "\n",
    "# if not os.path.exists(CHROMA_DB_PATH):\n",
    "#     print(f'Creating Chroma DB at {CHROMA_DB_PATH}...')\n",
    "chroma_db = Chroma.from_documents(source_chunks, OpenAIEmbeddings(), persist_directory=CHROMA_DB_PATH)\n",
    "chroma_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "qa = RetrievalQA(combine_documents_chain=qa_chain, retriever=chroma_db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qa.run(\"How can I load rasa model with an agent class/?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " You can use the `rasa.core.agent.Agent` class to load a trained Rasa model.\n",
       "\n",
       "The `load` method takes the path to the model as an argument.\n",
       "\n",
       "For example: `agent = Agent.load('./models/<model_name>.tar.gz')`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(res.replace('. ', '.\\n\\n')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
